---
title: "Regression of building energy data"
output:
  html_document: default
  pdf_document: default
---

```{r}
library(rjags)
library("ggplot2")
```

Loading data

```{r}
data <- read.csv("Energy_Efficiency.csv")
```

## Data Overview

Data consists of two responses and eight attributes:

• X1 Relative Compactness
• X2 Surface Area
• X3 Wall Area
• X4 Roof Area
• X5 Overall Height
• X6 Orientation (categorical)
• X7 Glazing Area 
• X8 Glazing Area Distribution (categorical)
• y1 Heating Load
• y2 Cooling Load

Renaming the columns to make them more understandable

```{r}
colnames(data) <- c('Relative_Compactness',
                      'Surface_Area',
                      'Wall_Area',
                      'Roof_Area',
                      'Overall_Height',
                      'Orientation', 
                      'Glazing_Area',
                      'Glazing_Area_Distribution',
                      'Heating_Load',
                      'Cooling_Load')

```


```{r}
head(data)
```


```{r}
summary(data) #basic statistical properties for the data
str(data)     #type of data and counts
```


Looking for Null-values in the data
```{r}
colSums(is.na(data))
```
No null-values found.



## Data Visualization

saving categorical predictors for plotting
```{r}
numerical_attributes <- c(
  'Relative_Compactness', 'Surface_Area', 'Wall_Area', 'Roof_Area',
  'Overall_Height', 'Glazing_Area'
)



```

###Histograms
Plotting histograms for both the response variables
```{r}
old_par <- par(mfrow = c(1, 2), mar = c(4, 4, 3, 2) + 0.1)
# histogram with added parameters
heating_load <-data[['Heating_Load']]

hist(heating_load,
main="Heating Load",
xlab="Heating Load [W]",
ylab="Frequenzy",
xlim=c(0,max(heating_load)+10),
col="darkmagenta"
)

cooling_load <-data[['Cooling_Load']]

hist(cooling_load,
main="Cooling Load",
xlab="Cooling Load [W]",
ylab="Frequenzy",
xlim=c(0,max(cooling_load)+10),
col="coral"
)




```



###Boxplots
plotting boxplots for data
```{r}
boxplot(data,col="lightblue", las = 2)
```
We notice that there is a scaling problem with the data. This can cause the regression to prioritize certain features. 
Solution: mean-centre the data.

Looking at individual boxplots, categorical values not included
```{r}

# Creating a plotting layout for a grid (2 rows, 4 columns)
old_par <- par(mfrow = c(2, 3), mar = c(4, 4, 3, 2) + 0.1)

# Looping through each numerical column to create a boxplot
for (col_name in numerical_attributes) {
  boxplot(data[[col_name]],
          main = paste(gsub("_", " ", col_name)), # Title for each plot
          ylab = "Value", # Y-axis label
          col = "lightblue", # Color of the boxplot
          las = 1 # Always horizontal axis labels for values
  )
}

par(old_par)
```
Also Roof Area, Relative Compactness and Glazing Area has a slight degree of skewness.  
Solution: use a prior distribution with a marked tail.

###Scatterplots

Categorical attributes not included

Scatterplots for Heat Load
```{r}
old_par <- par(mfrow = c(2, 3), mar = c(0, 0, 0, 0) + 3.8)


for (predictor_col in numerical_attributes) {
    plot(data[[predictor_col]], data[["Heating_Load"]],
         xlab = gsub("_", " ", predictor_col),  
         ylab = "Heating Load", 
         main = paste("Heating Load", "vs.", gsub("_", " ", predictor_col)), # Dynamic title
         col = "steelblue", # Point color
         pch = 16,          # Solid circles for points
         cex = 0.8,          # Size of points
         ylim = c(0, max(data[["Heating_Load"]]))
    )
}

```
We notice highly unlinear behaviour for Relative Compactness, and Surface Area


```{r}

old_par <- par(mfrow = c(2, 3), mar = c(0, 0, 0, 0) + 3.8)

for (predictor_col in numerical_attributes) {
    plot(data[[predictor_col]], data[["Cooling_Load"]],
         xlab = gsub("_", " ", predictor_col),  
         ylab = "Cooling Load", 
         main = paste("Cooling Load", "vs.", gsub("_", " ", predictor_col)), # Dynamic title
         col = "steelblue", # Point color
         pch = 16,          # Solid circles for points
         cex = 0.8,          # Size of points
         ylim = c(0, max(data[["Cooling_Load"]]))
    )
}



```


##Categorical Value Visualization

Analyzing orientation
```{r}
orientation_counts <- table(data[["Orientation"]]) #convert dataframe to vector  
print(orientation_counts)


barplot(orientation_counts,
        col = "orange",
        main = "Distribution of Orientations (X6)",
        xlab = "Orientation Category",
        ylab = "Frequency",
        xlim = c(0, 1),
        ylim = c(0, max(orientation_counts)+100),
        width = 0.1
        ) 
```

Analyzing Glazing Area Distribution
```{r}

glazing_area_counts <- table(data[["Glazing_Area_Distribution"]]) #convert dataframe to vector  
print(glazing_area_counts)

barplot(glazing_area_counts,
        col = "red",
        main = "Distribution of Glazing Area Distribution (X8)",
        xlab = "Orientation Category",
        ylab = "Frequency",
        xlim = c(0, 1),
        ylim = c(0, max(glazing_area_counts)+100),
        width = 0.1
        ) 
```

##Preprocessing


###Convert categorical attributes

To convert categorical attributes, replace categorical attributes with m-1 indicator attributes, where m = number of categories. One variable must be removed in order for X to be invertable
This can be automatically done with the as.factor function, https://www.rdocumentation.org/packages/h2o/versions/2.4.3.11/topics/as.factor

```{r}
transform_categorical_attributes <- function(data){

  categorical_attributes <- c('Orientation', 'Glazing_Area_Distribution')

  for(attribute in categorical_attributes){
    # Convert the column to a factor
    data[[attribute]] <- as.factor(data[[attribute]])
  }

  return(data) 
}

data <- transform_categorical_attributes(data)
```


###Mean Center
We only wish to mean center non-categorical attributes, mean centering will make the final model parameters more interpretative
```{r}

mean_center <- function(data) {
  unscaled_cols <- list('Orientation','Glazing_Area_Distribution','Heating_Load','Cooling_Load') #list of columns to not mean center (categorical + response vars)
  for(attribute in colnames(data)){
    if(attribute %in% unscaled_cols){next}
    data[attribute] <-  as.vector(scale(data[attribute])) #mean center the data
    }
  return(data)
}

data <- mean_center(data)
```


###Split into training/test set
```{r}
set.seed(42)

#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train  <- data[sample, ]
test   <- data[!sample, ]

formula_full <- Heating_Load ~ Relative_Compactness + Surface_Area + Wall_Area + Roof_Area + Overall_Height + Orientation + Glazing_Area + Glazing_Area_Distribution

# Training set
X <- model.matrix(formula_full, data = train)
y <- as.matrix(train["Heating_Load"])

#Test set
X_test <- model.matrix(formula_full, data = test)
y_test <- as.matrix(test["Heating_Load"])

```

##Model Selection


The goal of this sections is to perform linear regression in the Bayesian framework. The linear model is given by:
 
$$
y = X\beta+\epsilon  
$$
where
$$
\epsilon \sim \mathcal{N}(0, \sigma^2)
$$


The likelihood of $y$ given $\beta$, $\sigma^2$, and $X$ is normally distributed:

$$
y|\beta, \sigma^2, X \sim \mathcal{N}(X\beta, \sigma^2 I_n) \quad \text{likelihood}
$$

The prior distribution for the parameters $(\beta, \sigma^2)$ is given by $\pi(\beta, \sigma^2)$:

$$
(\beta, \sigma^2) \sim \pi(\beta, \sigma^2) \quad \text{prior}
$$
The objective of the analysis is to find good prior distributions.

###Conjugate prior
Conjugate priors have posteriors that can be easily calculated without simulation. However, there are many hyperparameters to tune. The prior distribution is the following:
$$
\beta|\sigma^2 \sim \mathcal{N}_{k+1}(\tilde{\beta}, \sigma^2 M^{-1}),
$$

$$
\sigma^2 \sim \mathcal{IG}(a, b)   
$$
The hyperparameters to tune are $a$, $b$ and $M$ where $M$ is a $(k-1)(k-1)$ matrix and $k$ is the number of attributes in the dataset.   

####Arbitrary values for hyper parameters

Testing a model with no information on the prior. Using $$M = I_{k+1}/c$$ to reduce the number of hyperparameters. Using large values on diagonal (imply high variance) to force the learning to come from the data 

Finding the posteriror analytically:
```{r}

# Get the dimension, note that X already have intercept-columns
k<-ncol(X) 
n<-nrow(X)

#defining hyperparameters:
a<-1    #low to get a flat distribution to account for uncertainty in prior 
b<-1    #low to get a flat distribution to account for uncertainty in prior
beta_pri <- rep(0, k) #zero to account for uncertainty in prior
c<-1000 #large to account for uncertainty in prior
M <- diag(k) * c

#calculating posterior
XtX <- t(X) %*% X
beta_hat <- solve(XtX) %*% t(X) %*% y
s2 <- t(y-X %*% beta_hat) %*% (y-X%*%beta_hat)

beta_post_cm <- solve(M + XtX) %*% (XtX %*% beta_hat+M %*% beta_pri)
M_post    <- solve(M+XtX)
a_post    <- n/2 + a
b_post    <- b + s2/2 + 1/2* (t(beta_post_cm-beta_hat) %*% solve(solve(M) + solve(XtX)) %*% (beta_post_cm-beta_hat))

```

### Testing Conjugate Model

```{r}
predictions_conjugate_model <- X_test %*% beta_post_cm

```


###Results
using Mean Square Error to see results
```{r}
mse <- mean((y_test - predictions_conjugate_model)^2)
cat("Test MSE:", mse, "\n")

```

##Model Testing
All models are trained on the same training-set, and a 80/20 split is used for training and test set validation. 





